# Zero Implementation Plan: Reports, Staleness Detection & Automation

**Version:** 1.2
**Date:** 2025-12-20
**Status:** ‚úÖ COMPLETE

---

## Executive Summary

This plan addressed key gaps in Zero to enable "always up-to-date" data:

1. **Phase 1: Shared Libraries** ‚úÖ - Built reusable foundation (`pkg/report/`, `pkg/findings/`, `pkg/freshness/`)
2. **Phase 2: Report Generation for All Scanners** ‚úÖ - Extended devx report pattern to all 9 scanners
3. **Phase 3: Staleness Detection & Freshness Metadata** ‚úÖ - Know when data is outdated
4. **Phase 4: Automation & Scheduled Scans** ‚úÖ - `zero watch`, `zero refresh` commands
5. **Phase 5: External Feed Sync & Rule Generation** ‚úÖ - `zero feeds` command (Semgrep rules only, OSV queried live)

**Implementation Complete:** All phases implemented with tests

---

## Phase 1: Shared Libraries

### Overview

Build shared libraries first to create a solid foundation. This reduces code duplication and avoids refactoring when implementing later phases.

### Why First?

- All 9 scanners will use consistent patterns from day one
- Report generation (Phase 2) goes faster since framework already exists
- No refactoring needed when adding new scanners later
- RAG and Live API infrastructure ready for Phase 5

### Current State

The codebase already has `pkg/scanners/common/` with:
- `exec.go` - Command execution utilities
- `json.go` - JSON helpers, severity normalization

The `devx` scanner has a full report implementation that should be extracted and shared.

### Implementation Tasks

#### Task 1.1: Report Generation Framework

**Files:** `pkg/report/builder.go`, `pkg/report/types.go`, `pkg/report/helpers.go`

Extract and enhance the report builder from `devx/report.go`:

```go
// pkg/report/builder.go
package report

import (
    "fmt"
    "strings"
    "time"
)

// ReportBuilder provides fluent API for markdown report generation
type ReportBuilder struct {
    sb strings.Builder
}

// NewBuilder creates a new report builder
func NewBuilder() *ReportBuilder {
    return &ReportBuilder{}
}

func (b *ReportBuilder) Title(title string) *ReportBuilder {
    b.sb.WriteString(fmt.Sprintf("# %s\n\n", title))
    return b
}

func (b *ReportBuilder) Meta(meta ReportMeta) *ReportBuilder {
    b.sb.WriteString(fmt.Sprintf("**Repository:** `%s`\n", meta.Repository))
    b.sb.WriteString(fmt.Sprintf("**Generated:** %s\n", meta.Timestamp.Format("2006-01-02 15:04:05 UTC")))
    b.sb.WriteString(fmt.Sprintf("**Scanner:** %s\n\n", meta.ScannerDesc))
    b.sb.WriteString("---\n\n")
    return b
}

func (b *ReportBuilder) Section(level int, title string) *ReportBuilder {
    prefix := strings.Repeat("#", level+1)
    b.sb.WriteString(fmt.Sprintf("%s %s\n\n", prefix, title))
    return b
}

func (b *ReportBuilder) Table(headers []string, rows [][]string) *ReportBuilder {
    // Header
    b.sb.WriteString("|")
    for _, h := range headers {
        b.sb.WriteString(fmt.Sprintf(" %s |", h))
    }
    b.sb.WriteString("\n|")
    for range headers {
        b.sb.WriteString("--------|")
    }
    b.sb.WriteString("\n")

    // Rows
    for _, row := range rows {
        b.sb.WriteString("|")
        for _, cell := range row {
            b.sb.WriteString(fmt.Sprintf(" %s |", cell))
        }
        b.sb.WriteString("\n")
    }
    b.sb.WriteString("\n")
    return b
}

func (b *ReportBuilder) Paragraph(text string) *ReportBuilder {
    b.sb.WriteString(text)
    b.sb.WriteString("\n\n")
    return b
}

func (b *ReportBuilder) List(items []string) *ReportBuilder {
    for _, item := range items {
        b.sb.WriteString(fmt.Sprintf("- %s\n", item))
    }
    b.sb.WriteString("\n")
    return b
}

func (b *ReportBuilder) NumberedList(items []string) *ReportBuilder {
    for i, item := range items {
        b.sb.WriteString(fmt.Sprintf("%d. %s\n", i+1, item))
    }
    b.sb.WriteString("\n")
    return b
}

func (b *ReportBuilder) CodeBlock(lang, code string) *ReportBuilder {
    b.sb.WriteString(fmt.Sprintf("```%s\n%s\n```\n\n", lang, code))
    return b
}

func (b *ReportBuilder) Quote(text string) *ReportBuilder {
    b.sb.WriteString(fmt.Sprintf("> %s\n\n", text))
    return b
}

func (b *ReportBuilder) Divider() *ReportBuilder {
    b.sb.WriteString("---\n\n")
    return b
}

func (b *ReportBuilder) Footer(scanner string) *ReportBuilder {
    b.sb.WriteString("---\n\n")
    b.sb.WriteString(fmt.Sprintf("*Generated by Zero %s Scanner*\n", scanner))
    return b
}

func (b *ReportBuilder) String() string {
    return b.sb.String()
}
```

```go
// pkg/report/types.go
package report

import "time"

// ReportType identifies the type of report
type ReportType string

const (
    TypeTechnical  ReportType = "technical"
    TypeExecutive  ReportType = "executive"
)

// ReportMeta contains common report metadata
type ReportMeta struct {
    Repository   string
    Timestamp    time.Time
    ScannerName  string
    ScannerDesc  string
}

// ReportConfig configures report generation
type ReportConfig struct {
    IncludeTechnical bool
    IncludeExecutive bool
    OutputDir        string
    Format           string // markdown, html, pdf
}
```

```go
// pkg/report/helpers.go
package report

import "strings"

// SeverityBadge returns a formatted severity badge
func SeverityBadge(severity string) string {
    switch strings.ToLower(severity) {
    case "critical":
        return "üî¥ CRITICAL"
    case "high":
        return "üü† HIGH"
    case "medium":
        return "üü° MEDIUM"
    case "low":
        return "üü¢ LOW"
    case "info":
        return "üîµ INFO"
    default:
        return severity
    }
}

// ScoreGrade converts a numeric score to a letter grade
func ScoreGrade(score int) string {
    switch {
    case score >= 90:
        return "A"
    case score >= 80:
        return "B"
    case score >= 70:
        return "C"
    case score >= 60:
        return "D"
    default:
        return "F"
    }
}

// ScoreStatus converts a numeric score to a status string
func ScoreStatus(score int) string {
    switch {
    case score >= 80:
        return "Excellent"
    case score >= 60:
        return "Good"
    case score >= 40:
        return "Needs Improvement"
    default:
        return "Critical"
    }
}

// BoolToCheckmark converts a bool to Yes/No
func BoolToCheckmark(b bool) string {
    if b {
        return "‚úÖ Yes"
    }
    return "‚ùå No"
}

// BoolToYesNo converts a bool to Yes/No (plain text)
func BoolToYesNo(b bool) string {
    if b {
        return "Yes"
    }
    return "No"
}

// Truncate truncates a string to maxLen with ellipsis
func Truncate(s string, maxLen int) string {
    if len(s) <= maxLen {
        return s
    }
    return s[:maxLen-3] + "..."
}
```

---

#### Task 1.2: Findings Framework

**Files:** `pkg/findings/types.go`, `pkg/findings/severity.go`, `pkg/findings/filter.go`

Standardize how all scanners report findings:

```go
// pkg/findings/types.go
package findings

import "time"

// Finding represents a single security or quality finding
type Finding struct {
    ID          string            `json:"id"`
    Title       string            `json:"title"`
    Description string            `json:"description"`
    Severity    Severity          `json:"severity"`
    Confidence  Confidence        `json:"confidence"`
    Category    string            `json:"category"`
    Scanner     string            `json:"scanner"`
    Location    *Location         `json:"location,omitempty"`
    Remediation string            `json:"remediation,omitempty"`
    References  []string          `json:"references,omitempty"`
    Metadata    map[string]any    `json:"metadata,omitempty"`
    CreatedAt   time.Time         `json:"created_at"`
}

// Location represents where a finding was detected
type Location struct {
    File      string `json:"file"`
    Line      int    `json:"line,omitempty"`
    EndLine   int    `json:"end_line,omitempty"`
    Column    int    `json:"column,omitempty"`
    EndColumn int    `json:"end_column,omitempty"`
    Snippet   string `json:"snippet,omitempty"`
}

// FindingSet represents a collection of findings with metadata
type FindingSet struct {
    Scanner     string    `json:"scanner"`
    Version     string    `json:"version"`
    Timestamp   time.Time `json:"timestamp"`
    Findings    []Finding `json:"findings"`
    Summary     Summary   `json:"summary"`
}

// Summary provides aggregated counts
type Summary struct {
    Total    int            `json:"total"`
    BySeverity map[Severity]int `json:"by_severity"`
    ByCategory map[string]int   `json:"by_category"`
}

// NewFindingSet creates a new finding set
func NewFindingSet(scanner string, findings []Finding) *FindingSet {
    fs := &FindingSet{
        Scanner:   scanner,
        Timestamp: time.Now(),
        Findings:  findings,
    }
    fs.computeSummary()
    return fs
}

func (fs *FindingSet) computeSummary() {
    fs.Summary = Summary{
        Total:      len(fs.Findings),
        BySeverity: make(map[Severity]int),
        ByCategory: make(map[string]int),
    }
    for _, f := range fs.Findings {
        fs.Summary.BySeverity[f.Severity]++
        fs.Summary.ByCategory[f.Category]++
    }
}
```

```go
// pkg/findings/severity.go
package findings

import "strings"

// Severity represents finding severity level
type Severity string

const (
    SeverityCritical Severity = "critical"
    SeverityHigh     Severity = "high"
    SeverityMedium   Severity = "medium"
    SeverityLow      Severity = "low"
    SeverityInfo     Severity = "info"
)

// Confidence represents finding confidence level
type Confidence string

const (
    ConfidenceHigh   Confidence = "high"
    ConfidenceMedium Confidence = "medium"
    ConfidenceLow    Confidence = "low"
)

// ParseSeverity normalizes severity strings
func ParseSeverity(s string) Severity {
    switch strings.ToLower(s) {
    case "critical", "crit":
        return SeverityCritical
    case "high":
        return SeverityHigh
    case "medium", "moderate", "med":
        return SeverityMedium
    case "low":
        return SeverityLow
    case "info", "informational", "note":
        return SeverityInfo
    default:
        return SeverityInfo
    }
}

// Score returns numeric score for sorting (higher = more severe)
func (s Severity) Score() int {
    switch s {
    case SeverityCritical:
        return 5
    case SeverityHigh:
        return 4
    case SeverityMedium:
        return 3
    case SeverityLow:
        return 2
    case SeverityInfo:
        return 1
    default:
        return 0
    }
}

// String returns the string representation
func (s Severity) String() string {
    return string(s)
}
```

```go
// pkg/findings/filter.go
package findings

import "sort"

// FilterOptions configures finding filtering
type FilterOptions struct {
    MinSeverity  Severity
    MaxSeverity  Severity
    Categories   []string
    Scanners     []string
    IncludeInfo  bool
    Limit        int
}

// Filter returns findings matching the options
func Filter(findings []Finding, opts FilterOptions) []Finding {
    var result []Finding

    for _, f := range findings {
        // Severity filter
        if opts.MinSeverity != "" && f.Severity.Score() < opts.MinSeverity.Score() {
            continue
        }
        if opts.MaxSeverity != "" && f.Severity.Score() > opts.MaxSeverity.Score() {
            continue
        }

        // Skip info unless explicitly included
        if !opts.IncludeInfo && f.Severity == SeverityInfo {
            continue
        }

        // Category filter
        if len(opts.Categories) > 0 && !contains(opts.Categories, f.Category) {
            continue
        }

        // Scanner filter
        if len(opts.Scanners) > 0 && !contains(opts.Scanners, f.Scanner) {
            continue
        }

        result = append(result, f)

        if opts.Limit > 0 && len(result) >= opts.Limit {
            break
        }
    }

    return result
}

// SortBySeverity sorts findings by severity (critical first)
func SortBySeverity(findings []Finding) []Finding {
    sorted := make([]Finding, len(findings))
    copy(sorted, findings)
    sort.Slice(sorted, func(i, j int) bool {
        return sorted[i].Severity.Score() > sorted[j].Severity.Score()
    })
    return sorted
}

// GroupByCategory groups findings by category
func GroupByCategory(findings []Finding) map[string][]Finding {
    groups := make(map[string][]Finding)
    for _, f := range findings {
        groups[f.Category] = append(groups[f.Category], f)
    }
    return groups
}

// GroupBySeverity groups findings by severity
func GroupBySeverity(findings []Finding) map[Severity][]Finding {
    groups := make(map[Severity][]Finding)
    for _, f := range findings {
        groups[f.Severity] = append(groups[f.Severity], f)
    }
    return groups
}

// Deduplicate removes duplicate findings based on ID
func Deduplicate(findings []Finding) []Finding {
    seen := make(map[string]bool)
    var result []Finding
    for _, f := range findings {
        if !seen[f.ID] {
            seen[f.ID] = true
            result = append(result, f)
        }
    }
    return result
}

func contains(slice []string, item string) bool {
    for _, s := range slice {
        if s == item {
            return true
        }
    }
    return false
}
```

---

#### Task 1.3: Scoring Framework

**Files:** `pkg/scoring/types.go`, `pkg/scoring/calculator.go`

Standardize score calculation across scanners:

```go
// pkg/scoring/types.go
package scoring

// Score represents a calculated score with metadata
type Score struct {
    Value      int              `json:"value"`      // 0-100
    Grade      string           `json:"grade"`      // A, B, C, D, F
    Level      string           `json:"level"`      // excellent, good, fair, poor, critical
    Components []ComponentScore `json:"components,omitempty"`
}

// ComponentScore represents a single component of a score
type ComponentScore struct {
    Name   string  `json:"name"`
    Value  int     `json:"value"`
    Weight float64 `json:"weight"`
    Max    int     `json:"max,omitempty"`
}

// RiskLevel represents overall risk assessment
type RiskLevel string

const (
    RiskCritical RiskLevel = "critical"
    RiskHigh     RiskLevel = "high"
    RiskMedium   RiskLevel = "medium"
    RiskLow      RiskLevel = "low"
    RiskNone     RiskLevel = "none"
)
```

```go
// pkg/scoring/calculator.go
package scoring

import "github.com/crashappsec/zero/pkg/findings"

// Calculate computes weighted score from components
func Calculate(components []ComponentScore) Score {
    if len(components) == 0 {
        return Score{Value: 0, Grade: "F", Level: "critical"}
    }

    var totalWeight float64
    var weightedSum float64

    for _, c := range components {
        totalWeight += c.Weight
        weightedSum += float64(c.Value) * c.Weight
    }

    value := 0
    if totalWeight > 0 {
        value = int(weightedSum / totalWeight)
    }

    return Score{
        Value:      value,
        Grade:      ValueToGrade(value),
        Level:      ValueToLevel(value),
        Components: components,
    }
}

// ValueToGrade converts a numeric score to letter grade
func ValueToGrade(value int) string {
    switch {
    case value >= 90:
        return "A"
    case value >= 80:
        return "B"
    case value >= 70:
        return "C"
    case value >= 60:
        return "D"
    default:
        return "F"
    }
}

// ValueToLevel converts a numeric score to a level string
func ValueToLevel(value int) string {
    switch {
    case value >= 90:
        return "excellent"
    case value >= 75:
        return "good"
    case value >= 50:
        return "fair"
    case value >= 25:
        return "poor"
    default:
        return "critical"
    }
}

// RiskFromFindings calculates risk level from findings
func RiskFromFindings(fs []findings.Finding) RiskLevel {
    var critical, high, medium int

    for _, f := range fs {
        switch f.Severity {
        case findings.SeverityCritical:
            critical++
        case findings.SeverityHigh:
            high++
        case findings.SeverityMedium:
            medium++
        }
    }

    switch {
    case critical > 0:
        return RiskCritical
    case high > 2:
        return RiskHigh
    case high > 0 || medium > 5:
        return RiskMedium
    case medium > 0:
        return RiskLow
    default:
        return RiskNone
    }
}

// SecurityScore calculates security score from findings
// Starts at 100 and deducts based on severity
func SecurityScore(fs []findings.Finding) Score {
    score := 100

    for _, f := range fs {
        switch f.Severity {
        case findings.SeverityCritical:
            score -= 25
        case findings.SeverityHigh:
            score -= 15
        case findings.SeverityMedium:
            score -= 5
        case findings.SeverityLow:
            score -= 1
        }
    }

    if score < 0 {
        score = 0
    }

    return Score{
        Value: score,
        Grade: ValueToGrade(score),
        Level: ValueToLevel(score),
    }
}
```

---

#### Task 1.4: RAG Pattern Loader

**Files:** `pkg/rag/loader.go`, `pkg/rag/semgrep.go`, `pkg/rag/types.go`

Extract and consolidate RAG loading from `tech-id`:

```go
// pkg/rag/types.go
package rag

// PatternSet represents a collection of patterns from RAG
type PatternSet struct {
    Category   string    `json:"category"`
    Technology string    `json:"technology"`
    Source     string    `json:"source"`
    Patterns   []Pattern `json:"patterns"`
}

// Pattern represents a single detection pattern
type Pattern struct {
    ID         string   `json:"id"`
    Type       string   `json:"type"`       // semgrep, regex, glob
    Pattern    string   `json:"pattern"`
    Message    string   `json:"message"`
    Severity   string   `json:"severity"`
    Confidence int      `json:"confidence"`
    Languages  []string `json:"languages"`
    Metadata   map[string]any `json:"metadata,omitempty"`
}

// RuleSet represents generated Semgrep rules
type RuleSet struct {
    Name       string `json:"name"`
    OutputFile string `json:"output_file"`
    RuleCount  int    `json:"rule_count"`
}
```

```go
// pkg/rag/loader.go
package rag

import (
    "fmt"
    "os"
    "path/filepath"
    "strings"
)

// Loader handles loading RAG patterns
type Loader struct {
    ragDir string
}

// NewLoader creates a new RAG loader
func NewLoader(ragDir string) *Loader {
    return &Loader{ragDir: ragDir}
}

// LoadPatterns loads patterns from a RAG category
func (l *Loader) LoadPatterns(category string) ([]PatternSet, error) {
    categoryDir := filepath.Join(l.ragDir, category)

    if _, err := os.Stat(categoryDir); os.IsNotExist(err) {
        return nil, fmt.Errorf("category not found: %s", category)
    }

    var patterns []PatternSet

    err := filepath.Walk(categoryDir, func(path string, info os.FileInfo, err error) error {
        if err != nil {
            return err
        }

        if info.IsDir() || !strings.HasSuffix(path, "patterns.md") {
            return nil
        }

        ps, err := l.parsePatternFile(path, category)
        if err != nil {
            return fmt.Errorf("parsing %s: %w", path, err)
        }

        patterns = append(patterns, ps)
        return nil
    })

    return patterns, err
}

func (l *Loader) parsePatternFile(path, category string) (PatternSet, error) {
    data, err := os.ReadFile(path)
    if err != nil {
        return PatternSet{}, err
    }

    // Extract technology name from path
    dir := filepath.Dir(path)
    tech := filepath.Base(dir)

    ps := PatternSet{
        Category:   category,
        Technology: tech,
        Source:     path,
    }

    // Parse markdown patterns
    ps.Patterns = parseMarkdownPatterns(string(data))

    return ps, nil
}

func parseMarkdownPatterns(content string) []Pattern {
    // Parse patterns from markdown format
    // This is a simplified version - the full implementation
    // would parse the actual RAG markdown format
    var patterns []Pattern
    // ... parsing logic
    return patterns
}
```

---

#### Task 1.5: Live API Client

**Files:** `pkg/liveapi/client.go`, `pkg/liveapi/osv.go`, `pkg/liveapi/cache.go`

Prepare infrastructure for live API queries (OSV):

```go
// pkg/liveapi/client.go
package liveapi

import (
    "bytes"
    "context"
    "encoding/json"
    "fmt"
    "io"
    "net/http"
    "time"
)

// Client is a generic API client with caching and rate limiting
type Client struct {
    BaseURL     string
    HTTPClient  *http.Client
    Cache       *Cache
    RateLimiter *RateLimiter
}

// NewClient creates a new API client
func NewClient(baseURL string, timeout time.Duration) *Client {
    return &Client{
        BaseURL: baseURL,
        HTTPClient: &http.Client{
            Timeout: timeout,
        },
        Cache:       NewCache(15 * time.Minute),
        RateLimiter: NewRateLimiter(10, time.Second), // 10 req/sec
    }
}

// Query performs a cached, rate-limited API query
func (c *Client) Query(ctx context.Context, path string, body any, result any) error {
    // Check cache first
    cacheKey := c.cacheKey(path, body)
    if cached, ok := c.Cache.Get(cacheKey); ok {
        return json.Unmarshal(cached, result)
    }

    // Wait for rate limiter
    if err := c.RateLimiter.Wait(ctx); err != nil {
        return fmt.Errorf("rate limit: %w", err)
    }

    // Make request
    url := c.BaseURL + path

    var reqBody io.Reader
    if body != nil {
        data, err := json.Marshal(body)
        if err != nil {
            return fmt.Errorf("marshal request: %w", err)
        }
        reqBody = bytes.NewReader(data)
    }

    req, err := http.NewRequestWithContext(ctx, "POST", url, reqBody)
    if err != nil {
        return fmt.Errorf("create request: %w", err)
    }
    req.Header.Set("Content-Type", "application/json")

    resp, err := c.HTTPClient.Do(req)
    if err != nil {
        return fmt.Errorf("request failed: %w", err)
    }
    defer resp.Body.Close()

    if resp.StatusCode != http.StatusOK {
        return fmt.Errorf("API error: %s", resp.Status)
    }

    respBody, err := io.ReadAll(resp.Body)
    if err != nil {
        return fmt.Errorf("read response: %w", err)
    }

    // Cache the response
    c.Cache.Set(cacheKey, respBody)

    return json.Unmarshal(respBody, result)
}

func (c *Client) cacheKey(path string, body any) string {
    data, _ := json.Marshal(body)
    return path + ":" + string(data)
}
```

```go
// pkg/liveapi/osv.go
package liveapi

import (
    "context"
    "time"
)

// Pre-approved URL for OSV API
const OSVBaseURL = "https://api.osv.dev/v1"

// OSVClient is a client for the OSV vulnerability database
type OSVClient struct {
    *Client
}

// NewOSVClient creates a new OSV client
func NewOSVClient(timeout time.Duration) *OSVClient {
    return &OSVClient{
        Client: NewClient(OSVBaseURL, timeout),
    }
}

// Vulnerability represents an OSV vulnerability
type Vulnerability struct {
    ID       string   `json:"id"`
    Summary  string   `json:"summary"`
    Details  string   `json:"details"`
    Severity string   `json:"severity"`
    Aliases  []string `json:"aliases"`
    Fixed    string   `json:"fixed,omitempty"`
}

// QueryRequest is the OSV query format
type QueryRequest struct {
    Package struct {
        Name      string `json:"name"`
        Ecosystem string `json:"ecosystem"`
    } `json:"package"`
    Version string `json:"version,omitempty"`
}

// QueryResponse is the OSV response format
type QueryResponse struct {
    Vulns []Vulnerability `json:"vulns"`
}

// QueryPackage queries OSV for vulnerabilities affecting a package
func (c *OSVClient) QueryPackage(ctx context.Context, ecosystem, name, version string) ([]Vulnerability, error) {
    req := QueryRequest{
        Version: version,
    }
    req.Package.Name = name
    req.Package.Ecosystem = ecosystem

    var resp QueryResponse
    if err := c.Query(ctx, "/query", req, &resp); err != nil {
        return nil, err
    }

    return resp.Vulns, nil
}

// QueryBatch queries multiple packages at once
func (c *OSVClient) QueryBatch(ctx context.Context, packages []QueryRequest) (map[string][]Vulnerability, error) {
    results := make(map[string][]Vulnerability)

    for _, pkg := range packages {
        key := pkg.Package.Ecosystem + ":" + pkg.Package.Name
        vulns, err := c.QueryPackage(ctx, pkg.Package.Ecosystem, pkg.Package.Name, pkg.Version)
        if err != nil {
            continue // Log but don't fail on individual package errors
        }
        results[key] = vulns
    }

    return results, nil
}
```

```go
// pkg/liveapi/cache.go
package liveapi

import (
    "sync"
    "time"
)

// Cache is a simple in-memory cache with TTL
type Cache struct {
    mu      sync.RWMutex
    items   map[string]cacheItem
    ttl     time.Duration
}

type cacheItem struct {
    data      []byte
    expiresAt time.Time
}

// NewCache creates a new cache with the given TTL
func NewCache(ttl time.Duration) *Cache {
    c := &Cache{
        items: make(map[string]cacheItem),
        ttl:   ttl,
    }
    go c.cleanup()
    return c
}

// Get retrieves an item from the cache
func (c *Cache) Get(key string) ([]byte, bool) {
    c.mu.RLock()
    defer c.mu.RUnlock()

    item, ok := c.items[key]
    if !ok || time.Now().After(item.expiresAt) {
        return nil, false
    }
    return item.data, true
}

// Set stores an item in the cache
func (c *Cache) Set(key string, data []byte) {
    c.mu.Lock()
    defer c.mu.Unlock()

    c.items[key] = cacheItem{
        data:      data,
        expiresAt: time.Now().Add(c.ttl),
    }
}

func (c *Cache) cleanup() {
    ticker := time.NewTicker(c.ttl / 2)
    for range ticker.C {
        c.mu.Lock()
        now := time.Now()
        for key, item := range c.items {
            if now.After(item.expiresAt) {
                delete(c.items, key)
            }
        }
        c.mu.Unlock()
    }
}
```

---

#### Task 1.6: Update Existing Scanners

Update existing code to use shared libraries:

1. **Update `pkg/scanners/common/json.go`** - Use `pkg/findings` severity types
2. **Update `pkg/scanners/devx/report.go`** - Use `pkg/report` builder
3. **Update `pkg/scanners/tech-id/rag_loader.go`** - Delegate to `pkg/rag`
4. **Update `pkg/scanners/tech-id/rag_converter.go`** - Delegate to `pkg/rag`

---

### Phase 1 Deliverables

| Deliverable | Files | Status |
|-------------|-------|--------|
| Report builder | `pkg/report/builder.go` | New |
| Report types | `pkg/report/types.go` | New |
| Report helpers | `pkg/report/helpers.go` | New |
| Findings types | `pkg/findings/types.go` | New |
| Severity handling | `pkg/findings/severity.go` | New |
| Finding filters | `pkg/findings/filter.go` | New |
| Scoring types | `pkg/scoring/types.go` | New |
| Score calculator | `pkg/scoring/calculator.go` | New |
| RAG types | `pkg/rag/types.go` | New |
| RAG loader | `pkg/rag/loader.go` | New |
| RAG semgrep | `pkg/rag/semgrep.go` | New |
| Live API client | `pkg/liveapi/client.go` | New |
| OSV client | `pkg/liveapi/osv.go` | New |
| API cache | `pkg/liveapi/cache.go` | New |
| Scanner updates | Multiple | Modify |

**Estimated Effort:** 2-3 days

---

## Phase 2: Report Generation for All Scanners

### Overview

Now that shared libraries exist, implement reports for all 8 remaining scanners. This is faster since the framework is already built.

### Current State (COMPLETED)

| Scanner | JSON Output | Technical Report | Executive Report |
|---------|-------------|------------------|------------------|
| sbom | ‚úÖ | ‚úÖ | ‚úÖ |
| package-analysis | ‚úÖ | ‚úÖ | ‚úÖ |
| crypto | ‚úÖ | ‚úÖ | ‚úÖ |
| code-security | ‚úÖ | ‚úÖ | ‚úÖ |
| code-quality | ‚úÖ | ‚úÖ | ‚úÖ |
| devops | ‚úÖ | ‚úÖ | ‚úÖ |
| tech-id | ‚úÖ | ‚úÖ | ‚úÖ |
| code-ownership | ‚úÖ | ‚úÖ | ‚úÖ |
| devx | ‚úÖ | ‚úÖ | ‚úÖ |

### Architecture

Each scanner will have a `report.go` file following the devx pattern:

```go
// pkg/scanners/{scanner}/report.go

type ReportData struct {
    Repository string
    Timestamp  time.Time
    Summary    Summary
    Findings   Findings
}

func LoadReportData(analysisDir string) (*ReportData, error)
func GenerateTechnicalReport(data *ReportData) string
func GenerateExecutiveReport(data *ReportData) string
func WriteReports(analysisDir string) error
```

### Implementation Tasks

#### Task 2.1: SBOM Scanner Report (Using Shared Framework)

**Note:** The report framework was already created in Phase 1 (Task 1.1). Now we use it.

**File:** `pkg/report/generator.go`

Reference shared report generation framework:

```go
package report

import (
    "fmt"
    "strings"
    "time"
)

// ReportType identifies the type of report
type ReportType string

const (
    TypeTechnical  ReportType = "technical"
    TypeExecutive  ReportType = "executive"
)

// ReportMeta contains common report metadata
type ReportMeta struct {
    Repository   string
    Timestamp    time.Time
    ScannerName  string
    ScannerDesc  string
}

// ReportBuilder helps construct markdown reports
type ReportBuilder struct {
    sb strings.Builder
}

func NewBuilder() *ReportBuilder {
    return &ReportBuilder{}
}

func (b *ReportBuilder) Title(title string) {
    b.sb.WriteString(fmt.Sprintf("# %s\n\n", title))
}

func (b *ReportBuilder) Meta(meta ReportMeta) {
    b.sb.WriteString(fmt.Sprintf("**Repository:** `%s`\n", meta.Repository))
    b.sb.WriteString(fmt.Sprintf("**Generated:** %s\n", meta.Timestamp.Format("2006-01-02 15:04:05 UTC")))
    b.sb.WriteString(fmt.Sprintf("**Scanner:** %s\n\n", meta.ScannerDesc))
    b.sb.WriteString("---\n\n")
}

func (b *ReportBuilder) Section(level int, title string) {
    prefix := strings.Repeat("#", level+1)
    b.sb.WriteString(fmt.Sprintf("%s %s\n\n", prefix, title))
}

func (b *ReportBuilder) Table(headers []string, rows [][]string) {
    // Header
    b.sb.WriteString("|")
    for _, h := range headers {
        b.sb.WriteString(fmt.Sprintf(" %s |", h))
    }
    b.sb.WriteString("\n|")
    for range headers {
        b.sb.WriteString("--------|")
    }
    b.sb.WriteString("\n")

    // Rows
    for _, row := range rows {
        b.sb.WriteString("|")
        for _, cell := range row {
            b.sb.WriteString(fmt.Sprintf(" %s |", cell))
        }
        b.sb.WriteString("\n")
    }
    b.sb.WriteString("\n")
}

func (b *ReportBuilder) Paragraph(text string) {
    b.sb.WriteString(text)
    b.sb.WriteString("\n\n")
}

func (b *ReportBuilder) List(items []string) {
    for _, item := range items {
        b.sb.WriteString(fmt.Sprintf("- %s\n", item))
    }
    b.sb.WriteString("\n")
}

func (b *ReportBuilder) SeverityBadge(severity string) string {
    switch strings.ToLower(severity) {
    case "critical":
        return "üî¥ CRITICAL"
    case "high":
        return "üü† HIGH"
    case "medium":
        return "üü° MEDIUM"
    case "low":
        return "üü¢ LOW"
    default:
        return severity
    }
}

func (b *ReportBuilder) ScoreGrade(score int) string {
    switch {
    case score >= 90:
        return "A"
    case score >= 80:
        return "B"
    case score >= 70:
        return "C"
    case score >= 60:
        return "D"
    default:
        return "F"
    }
}

func (b *ReportBuilder) Footer() {
    b.sb.WriteString("---\n\n")
    b.sb.WriteString("*Generated by Zero Security Scanner*\n")
}

func (b *ReportBuilder) String() string {
    return b.sb.String()
}
```

---

#### Task 2.2: SBOM Scanner Report

**File:** `pkg/scanners/sbom/report.go`

**Technical Report Content:**
- Package breakdown by ecosystem (npm, pypi, go, etc.)
- Direct vs transitive dependencies
- Package version distribution
- Dependency tree depth
- SBOM integrity status

**Executive Report Content:**
- Total dependency count
- Ecosystem distribution summary
- Key risks (large dependency trees, many transitive deps)

```go
package sbom

import (
    "encoding/json"
    "fmt"
    "os"
    "path/filepath"
    "time"

    "github.com/crashappsec/zero/pkg/report"
)

type ReportData struct {
    Repository string
    Timestamp  time.Time
    Summary    Summary
    Findings   struct {
        Packages []Package
    }
}

func LoadReportData(analysisDir string) (*ReportData, error) {
    path := filepath.Join(analysisDir, "sbom.json")
    data, err := os.ReadFile(path)
    if err != nil {
        return nil, fmt.Errorf("reading sbom.json: %w", err)
    }

    var result ReportData
    if err := json.Unmarshal(data, &result); err != nil {
        return nil, fmt.Errorf("parsing sbom.json: %w", err)
    }
    return &result, nil
}

func GenerateTechnicalReport(data *ReportData) string {
    b := report.NewBuilder()

    b.Title("SBOM Technical Report")
    b.Meta(report.ReportMeta{
        Repository:  data.Repository,
        Timestamp:   data.Timestamp,
        ScannerName: "sbom",
        ScannerDesc: "Software Bill of Materials Generator",
    })

    // Summary section
    b.Section(1, "Summary")
    b.Table(
        []string{"Metric", "Value"},
        [][]string{
            {"Total Packages", fmt.Sprintf("%d", data.Summary.TotalPackages)},
            {"Direct Dependencies", fmt.Sprintf("%d", data.Summary.DirectDeps)},
            {"Transitive Dependencies", fmt.Sprintf("%d", data.Summary.TransitiveDeps)},
            {"Ecosystems", fmt.Sprintf("%d", len(data.Summary.ByEcosystem))},
        },
    )

    // Ecosystem breakdown
    if len(data.Summary.ByEcosystem) > 0 {
        b.Section(1, "Ecosystem Breakdown")
        var rows [][]string
        for eco, count := range data.Summary.ByEcosystem {
            pct := float64(count) / float64(data.Summary.TotalPackages) * 100
            rows = append(rows, []string{eco, fmt.Sprintf("%d", count), fmt.Sprintf("%.1f%%", pct)})
        }
        b.Table([]string{"Ecosystem", "Packages", "Percentage"}, rows)
    }

    b.Footer()
    return b.String()
}

func GenerateExecutiveReport(data *ReportData) string {
    b := report.NewBuilder()

    b.Title("SBOM Executive Summary")
    b.Meta(report.ReportMeta{
        Repository:  data.Repository,
        Timestamp:   data.Timestamp,
        ScannerName: "sbom",
        ScannerDesc: "Software Bill of Materials",
    })

    // Key metrics
    b.Section(1, "Dependency Overview")
    b.Paragraph(fmt.Sprintf(
        "This repository has **%d total dependencies** across **%d ecosystems**.",
        data.Summary.TotalPackages,
        len(data.Summary.ByEcosystem),
    ))

    // Risk assessment
    b.Section(1, "Risk Indicators")
    risks := []string{}
    if data.Summary.TransitiveDeps > data.Summary.DirectDeps*5 {
        risks = append(risks, "High transitive dependency ratio - increased supply chain risk")
    }
    if data.Summary.TotalPackages > 500 {
        risks = append(risks, "Large dependency footprint - consider dependency audit")
    }
    if len(risks) > 0 {
        b.List(risks)
    } else {
        b.Paragraph("No significant risk indicators detected.")
    }

    b.Footer()
    return b.String()
}

func WriteReports(analysisDir string) error {
    data, err := LoadReportData(analysisDir)
    if err != nil {
        return err
    }

    // Technical report
    techReport := GenerateTechnicalReport(data)
    if err := os.WriteFile(filepath.Join(analysisDir, "sbom-technical-report.md"), []byte(techReport), 0644); err != nil {
        return fmt.Errorf("writing technical report: %w", err)
    }

    // Executive report
    execReport := GenerateExecutiveReport(data)
    if err := os.WriteFile(filepath.Join(analysisDir, "sbom-executive-report.md"), []byte(execReport), 0644); err != nil {
        return fmt.Errorf("writing executive report: %w", err)
    }

    return nil
}
```

---

#### Task 2.3: Package Analysis Scanner Report

**File:** `pkg/scanners/package-analysis/report.go`

**Technical Report Content:**
- Vulnerability breakdown by severity
- Vulnerability details (CVE, package, version, fix)
- Health issues (deprecated, unmaintained, abandoned)
- License compliance status
- Malcontent findings
- Typosquatting risks

**Executive Report Content:**
- Risk score
- Critical/high vulnerability count
- Actionable remediation priorities
- License compatibility summary

---

#### Task 2.4: Crypto Scanner Report

**File:** `pkg/scanners/crypto/report.go`

**Technical Report Content:**
- Weak cipher findings with file:line references
- Hardcoded key findings
- TLS configuration issues
- Weak RNG usage
- Certificate analysis

**Executive Report Content:**
- Cryptographic risk score
- Critical crypto issues count
- Remediation priorities

---

#### Task 2.5: Code Security Scanner Report

**File:** `pkg/scanners/code-security/report.go`

**Technical Report Content:**
- SAST findings by severity and category
- Secret detection results
- API security issues
- Code vulnerability details with remediation

**Executive Report Content:**
- Security posture score
- Critical vulnerability summary
- Top remediation priorities

---

#### Task 2.6: Code Quality Scanner Report

**File:** `pkg/scanners/code-quality/report.go`

**Technical Report Content:**
- Technical debt markers (TODO, FIXME, HACK counts)
- Complexity metrics
- Test coverage analysis
- Documentation coverage

**Executive Report Content:**
- Code quality grade
- Technical debt summary
- Test coverage percentage
- Key improvement areas

---

#### Task 2.7: DevOps Scanner Report

**File:** `pkg/scanners/devops/report.go`

**Technical Report Content:**
- IaC security findings
- Container security issues
- GitHub Actions analysis
- DORA metrics
- Git repository metrics

**Executive Report Content:**
- DevOps maturity score
- DORA classification (Elite/High/Medium/Low)
- CI/CD security summary
- Deployment frequency insights

---

#### Task 2.8: Tech-ID Scanner Report

**File:** `pkg/scanners/tech-id/report.go`

**Technical Report Content:**
- Technology stack breakdown
- ML/AI model inventory (ML-BOM)
- Framework detection
- Dataset analysis
- AI security findings

**Executive Report Content:**
- Technology portfolio summary
- ML/AI adoption status
- Key technology risks

---

#### Task 2.9: Code Ownership Scanner Report

**File:** `pkg/scanners/code-ownership/report.go`

**Technical Report Content:**
- Contributor analysis
- Bus factor calculation
- CODEOWNERS coverage
- Orphaned files
- Code churn patterns
- Activity trends

**Executive Report Content:**
- Team health indicators
- Bus factor risk assessment
- Ownership coverage
- Key recommendations

---

#### Task 2.10: Unified Report Command

**File:** `pkg/report/unified.go`

Add a unified report generator that can produce combined reports:

```go
package report

import (
    "fmt"
    "os"
    "path/filepath"

    // Import all scanner report packages
    "github.com/crashappsec/zero/pkg/scanners/sbom"
    pkganalysis "github.com/crashappsec/zero/pkg/scanners/package-analysis"
    "github.com/crashappsec/zero/pkg/scanners/crypto"
    codesec "github.com/crashappsec/zero/pkg/scanners/code-security"
    codequal "github.com/crashappsec/zero/pkg/scanners/code-quality"
    "github.com/crashappsec/zero/pkg/scanners/devops"
    techid "github.com/crashappsec/zero/pkg/scanners/tech-id"
    ownership "github.com/crashappsec/zero/pkg/scanners/code-ownership"
    "github.com/crashappsec/zero/pkg/scanners/devx"
)

// GenerateAllReports generates reports for all scanners
func GenerateAllReports(analysisDir string) error {
    generators := []func(string) error{
        sbom.WriteReports,
        pkganalysis.WriteReports,
        crypto.WriteReports,
        codesec.WriteReports,
        codequal.WriteReports,
        devops.WriteReports,
        techid.WriteReports,
        ownership.WriteReports,
        devx.WriteReports,
    }

    var errors []error
    for _, gen := range generators {
        if err := gen(analysisDir); err != nil {
            errors = append(errors, err)
        }
    }

    if len(errors) > 0 {
        return fmt.Errorf("report generation had %d errors", len(errors))
    }
    return nil
}

// GenerateCombinedExecutiveReport creates a single executive summary
func GenerateCombinedExecutiveReport(analysisDir, repository string) (string, error) {
    b := NewBuilder()

    b.Title("Security & Quality Executive Report")
    b.Paragraph(fmt.Sprintf("**Repository:** `%s`", repository))
    b.Paragraph("---")

    // Load and summarize each scanner's data
    // ... aggregate findings into unified view

    return b.String(), nil
}
```

---

#### Task 2.11: CLI Integration

**File:** `cmd/zero/cmd/report.go`

Update the report command to support the new report types:

```go
// Add new flags
reportCmd.Flags().String("scanner", "", "Generate report for specific scanner")
reportCmd.Flags().Bool("technical", false, "Generate technical reports")
reportCmd.Flags().Bool("executive", false, "Generate executive reports")
reportCmd.Flags().Bool("all", false, "Generate all report types")
reportCmd.Flags().Bool("combined", false, "Generate combined executive summary")
```

---

### Phase 2 Deliverables

| Deliverable | Files | Status |
|-------------|-------|--------|
| SBOM reports | `pkg/scanners/sbom/report.go` | New |
| Package analysis reports | `pkg/scanners/package-analysis/report.go` | New |
| Crypto reports | `pkg/scanners/crypto/report.go` | New |
| Code security reports | `pkg/scanners/code-security/report.go` | New |
| Code quality reports | `pkg/scanners/code-quality/report.go` | New |
| DevOps reports | `pkg/scanners/devops/report.go` | New |
| Tech-ID reports | `pkg/scanners/tech-id/report.go` | New |
| Code ownership reports | `pkg/scanners/code-ownership/report.go` | New |
| Unified generator | `pkg/report/unified.go` | New |
| CLI updates | `cmd/zero/cmd/report.go` | Modify |

**Note:** Report framework was created in Phase 1. This phase uses the shared libraries.

**Estimated Effort:** 2-4 days (faster due to shared libs from Phase 1)

---

## Phase 3: Staleness Detection & Freshness Metadata

### Overview

Add metadata tracking to detect when analysis data is outdated, enabling users to know when re-scans are needed.

### Architecture

```
.zero/repos/{project}/
‚îú‚îÄ‚îÄ repo/                    # Cloned repository
‚îú‚îÄ‚îÄ analysis/                # Current scan results
‚îú‚îÄ‚îÄ history/                 # Historical scans
‚îî‚îÄ‚îÄ freshness.json           # NEW: Freshness metadata
```

### Implementation Tasks

#### Task 3.1: Freshness Types

**File:** `pkg/freshness/types.go`

```go
package freshness

import "time"

// StalenessLevel indicates how stale the data is
type StalenessLevel string

const (
    Fresh     StalenessLevel = "fresh"       // < 24 hours, no new commits
    Stale     StalenessLevel = "stale"       // 1-7 days or 1-10 new commits
    VeryStale StalenessLevel = "very-stale"  // > 7 days or > 10 new commits
    Unknown   StalenessLevel = "unknown"     // Cannot determine
)

// FreshnessMetadata tracks when data was last updated
type FreshnessMetadata struct {
    // Project identification
    ProjectID string `json:"project_id"`

    // Clone timestamps
    ClonedAt       time.Time `json:"cloned_at"`
    LastFetchedAt  time.Time `json:"last_fetched_at,omitempty"`

    // Scan timestamps
    LastScannedAt  time.Time `json:"last_scanned_at"`
    ScanDuration   string    `json:"scan_duration"`
    ScanProfile    string    `json:"scan_profile"`

    // Git state at last scan
    LocalCommit    string    `json:"local_commit"`
    LocalBranch    string    `json:"local_branch"`

    // Remote state (populated on freshness check)
    RemoteCommit   string    `json:"remote_commit,omitempty"`
    NewCommitCount int       `json:"new_commit_count,omitempty"`
    RemoteCheckedAt time.Time `json:"remote_checked_at,omitempty"`

    // Computed freshness
    StalenessLevel StalenessLevel `json:"staleness_level"`
    StalenessReason string        `json:"staleness_reason,omitempty"`

    // Scanner versions (for invalidation)
    ScannerVersions map[string]string `json:"scanner_versions,omitempty"`

    // Data integrity
    AnalysisHash   string    `json:"analysis_hash,omitempty"`
}

// FreshnessConfig controls freshness thresholds
type FreshnessConfig struct {
    // Time-based thresholds
    FreshMaxAge     time.Duration `json:"fresh_max_age"`      // Default: 24h
    StaleMaxAge     time.Duration `json:"stale_max_age"`      // Default: 7d

    // Commit-based thresholds
    FreshMaxCommits int           `json:"fresh_max_commits"`  // Default: 0
    StaleMaxCommits int           `json:"stale_max_commits"`  // Default: 10

    // Auto-refresh settings
    AutoRefreshOnStale bool       `json:"auto_refresh_on_stale"`
    CheckRemoteOnStatus bool      `json:"check_remote_on_status"`
}

// DefaultConfig returns the default freshness configuration
func DefaultConfig() FreshnessConfig {
    return FreshnessConfig{
        FreshMaxAge:         24 * time.Hour,
        StaleMaxAge:         7 * 24 * time.Hour,
        FreshMaxCommits:     0,
        StaleMaxCommits:     10,
        AutoRefreshOnStale:  false,
        CheckRemoteOnStatus: true,
    }
}
```

---

#### Task 3.2: Freshness Manager

**File:** `pkg/freshness/manager.go`

```go
package freshness

import (
    "crypto/sha256"
    "encoding/hex"
    "encoding/json"
    "fmt"
    "io"
    "os"
    "os/exec"
    "path/filepath"
    "strings"
    "time"
)

// Manager handles freshness operations
type Manager struct {
    zeroHome string
    config   FreshnessConfig
}

// NewManager creates a new freshness manager
func NewManager(zeroHome string, config FreshnessConfig) *Manager {
    return &Manager{
        zeroHome: zeroHome,
        config:   config,
    }
}

// GetFreshnessFile returns the path to freshness.json
func (m *Manager) GetFreshnessFile(projectID string) string {
    return filepath.Join(m.zeroHome, "repos", projectID, "freshness.json")
}

// Load loads freshness metadata for a project
func (m *Manager) Load(projectID string) (*FreshnessMetadata, error) {
    path := m.GetFreshnessFile(projectID)

    data, err := os.ReadFile(path)
    if err != nil {
        if os.IsNotExist(err) {
            return nil, nil // No metadata yet
        }
        return nil, fmt.Errorf("reading freshness file: %w", err)
    }

    var meta FreshnessMetadata
    if err := json.Unmarshal(data, &meta); err != nil {
        return nil, fmt.Errorf("parsing freshness file: %w", err)
    }

    return &meta, nil
}

// Save saves freshness metadata for a project
func (m *Manager) Save(projectID string, meta *FreshnessMetadata) error {
    path := m.GetFreshnessFile(projectID)

    data, err := json.MarshalIndent(meta, "", "  ")
    if err != nil {
        return fmt.Errorf("marshaling freshness: %w", err)
    }

    if err := os.WriteFile(path, data, 0644); err != nil {
        return fmt.Errorf("writing freshness file: %w", err)
    }

    return nil
}

// RecordScan records a new scan in freshness metadata
func (m *Manager) RecordScan(projectID, commit, branch, profile string, duration time.Duration) error {
    meta := &FreshnessMetadata{
        ProjectID:       projectID,
        LastScannedAt:   time.Now(),
        ScanDuration:    duration.String(),
        ScanProfile:     profile,
        LocalCommit:     commit,
        LocalBranch:     branch,
        StalenessLevel:  Fresh,
    }

    // Load existing to preserve clone time
    existing, _ := m.Load(projectID)
    if existing != nil {
        meta.ClonedAt = existing.ClonedAt
    } else {
        meta.ClonedAt = time.Now()
    }

    // Compute analysis hash
    analysisDir := filepath.Join(m.zeroHome, "repos", projectID, "analysis")
    hash, _ := m.computeAnalysisHash(analysisDir)
    meta.AnalysisHash = hash

    return m.Save(projectID, meta)
}

// CheckFreshness evaluates the current freshness of a project
func (m *Manager) CheckFreshness(projectID string, checkRemote bool) (*FreshnessMetadata, error) {
    meta, err := m.Load(projectID)
    if err != nil {
        return nil, err
    }

    if meta == nil {
        return &FreshnessMetadata{
            ProjectID:      projectID,
            StalenessLevel: Unknown,
            StalenessReason: "No scan data available",
        }, nil
    }

    // Check remote for new commits if requested
    if checkRemote {
        repoPath := filepath.Join(m.zeroHome, "repos", projectID, "repo")
        remoteCommit, newCommits, err := m.checkRemote(repoPath, meta.LocalCommit)
        if err == nil {
            meta.RemoteCommit = remoteCommit
            meta.NewCommitCount = newCommits
            meta.RemoteCheckedAt = time.Now()
        }
    }

    // Compute staleness
    m.computeStaleness(meta)

    // Save updated metadata
    m.Save(projectID, meta)

    return meta, nil
}

// computeStaleness calculates the staleness level
func (m *Manager) computeStaleness(meta *FreshnessMetadata) {
    age := time.Since(meta.LastScannedAt)

    // Check time-based staleness
    if age > m.config.StaleMaxAge {
        meta.StalenessLevel = VeryStale
        meta.StalenessReason = fmt.Sprintf("Last scan was %s ago", formatDuration(age))
        return
    }

    // Check commit-based staleness
    if meta.NewCommitCount > m.config.StaleMaxCommits {
        meta.StalenessLevel = VeryStale
        meta.StalenessReason = fmt.Sprintf("%d new commits since last scan", meta.NewCommitCount)
        return
    }

    if age > m.config.FreshMaxAge || meta.NewCommitCount > m.config.FreshMaxCommits {
        meta.StalenessLevel = Stale
        if meta.NewCommitCount > 0 {
            meta.StalenessReason = fmt.Sprintf("%d new commits, last scan %s ago",
                meta.NewCommitCount, formatDuration(age))
        } else {
            meta.StalenessReason = fmt.Sprintf("Last scan was %s ago", formatDuration(age))
        }
        return
    }

    meta.StalenessLevel = Fresh
    meta.StalenessReason = ""
}

// checkRemote fetches remote and counts new commits
func (m *Manager) checkRemote(repoPath, localCommit string) (string, int, error) {
    // Fetch without updating
    cmd := exec.Command("git", "fetch", "--dry-run")
    cmd.Dir = repoPath
    cmd.Run() // Ignore errors, just updates refs

    // Get remote HEAD
    cmd = exec.Command("git", "rev-parse", "origin/HEAD")
    cmd.Dir = repoPath
    out, err := cmd.Output()
    if err != nil {
        return "", 0, err
    }
    remoteCommit := strings.TrimSpace(string(out))

    // Count commits between local and remote
    cmd = exec.Command("git", "rev-list", "--count", localCommit+".."+remoteCommit)
    cmd.Dir = repoPath
    out, err = cmd.Output()
    if err != nil {
        return remoteCommit, 0, nil
    }

    var count int
    fmt.Sscanf(strings.TrimSpace(string(out)), "%d", &count)

    return remoteCommit, count, nil
}

// computeAnalysisHash creates a hash of all analysis files
func (m *Manager) computeAnalysisHash(analysisDir string) (string, error) {
    h := sha256.New()

    err := filepath.Walk(analysisDir, func(path string, info os.FileInfo, err error) error {
        if err != nil || info.IsDir() {
            return err
        }

        if !strings.HasSuffix(path, ".json") {
            return nil
        }

        f, err := os.Open(path)
        if err != nil {
            return err
        }
        defer f.Close()

        io.Copy(h, f)
        return nil
    })

    if err != nil {
        return "", err
    }

    return hex.EncodeToString(h.Sum(nil))[:16], nil
}

func formatDuration(d time.Duration) string {
    if d < time.Hour {
        return fmt.Sprintf("%d minutes", int(d.Minutes()))
    }
    if d < 24*time.Hour {
        return fmt.Sprintf("%d hours", int(d.Hours()))
    }
    return fmt.Sprintf("%d days", int(d.Hours()/24))
}
```

---

#### Task 3.3: CLI Status Command Update

**File:** `cmd/zero/cmd/status.go`

Update the status command to show freshness:

```go
// Add freshness display to status output
func showProjectStatus(projectID string) {
    // ... existing code ...

    // Load freshness
    fm := freshness.NewManager(zeroHome, freshness.DefaultConfig())
    meta, err := fm.CheckFreshness(projectID, true)
    if err == nil && meta != nil {
        switch meta.StalenessLevel {
        case freshness.Fresh:
            fmt.Printf("  Freshness: %s\n", term.Color(terminal.Green, "FRESH"))
        case freshness.Stale:
            fmt.Printf("  Freshness: %s (%s)\n",
                term.Color(terminal.Yellow, "STALE"),
                meta.StalenessReason)
        case freshness.VeryStale:
            fmt.Printf("  Freshness: %s (%s)\n",
                term.Color(terminal.Red, "VERY STALE"),
                meta.StalenessReason)
        }

        fmt.Printf("  Last Scan: %s\n", meta.LastScannedAt.Format("2006-01-02 15:04"))

        if meta.NewCommitCount > 0 {
            fmt.Printf("  New Commits: %d\n", meta.NewCommitCount)
        }
    }
}
```

---

#### Task 3.4: Hydrate Integration

**File:** `pkg/hydrate/hydrate.go`

Update hydrate to record freshness metadata after successful scans:

```go
// In the Run() function, after successful scan:
func (h *Hydrate) recordFreshness(projectID, profile string, duration time.Duration) error {
    repoPath := filepath.Join(h.zeroHome, "repos", projectID, "repo")

    // Get current commit
    cmd := exec.Command("git", "rev-parse", "HEAD")
    cmd.Dir = repoPath
    out, err := cmd.Output()
    if err != nil {
        return err
    }
    commit := strings.TrimSpace(string(out))

    // Get current branch
    cmd = exec.Command("git", "rev-parse", "--abbrev-ref", "HEAD")
    cmd.Dir = repoPath
    out, _ = cmd.Output()
    branch := strings.TrimSpace(string(out))

    fm := freshness.NewManager(h.zeroHome, freshness.DefaultConfig())
    return fm.RecordScan(projectID, commit, branch, profile, duration)
}
```

---

#### Task 3.5: Conditional Hydrate (--if-stale)

**File:** `cmd/zero/cmd/hydrate.go`

Add a flag to only hydrate if data is stale:

```go
hydrateCmd.Flags().Bool("if-stale", false, "Only re-scan if data is stale")
hydrateCmd.Flags().String("staleness", "stale", "Staleness threshold: stale or very-stale")

// In Run function:
if ifStale {
    fm := freshness.NewManager(zeroHome, freshness.DefaultConfig())
    meta, _ := fm.CheckFreshness(projectID, true)

    if meta != nil && meta.StalenessLevel == freshness.Fresh {
        fmt.Printf("Skipping %s - data is fresh\n", projectID)
        return nil
    }

    if stalenessThreshold == "very-stale" && meta.StalenessLevel != freshness.VeryStale {
        fmt.Printf("Skipping %s - data is not very stale\n", projectID)
        return nil
    }
}
```

---

### Phase 3 Deliverables

| Deliverable | Files | Status |
|-------------|-------|--------|
| Freshness types | `pkg/freshness/types.go` | New |
| Freshness manager | `pkg/freshness/manager.go` | New |
| Status command update | `cmd/zero/cmd/status.go` | Modify |
| Hydrate integration | `pkg/hydrate/hydrate.go` | Modify |
| Conditional hydrate | `cmd/zero/cmd/hydrate.go` | Modify |
| Config integration | `pkg/config/config.go` | Modify |

**Estimated Effort:** 2-3 days

---

## Phase 4: Automation & Scheduled Scans

### Overview

Enable automatic scanning through GitHub Actions, webhooks, and watch modes.

### Implementation Tasks

#### Task 4.1: GitHub Actions Workflow

**File:** `.github/workflows/zero-scan.yml`

```yaml
name: Zero Security Scan

on:
  schedule:
    - cron: '0 2 * * *'  # Daily at 2 AM UTC
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  workflow_dispatch:
    inputs:
      profile:
        description: 'Scan profile'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - security
          - full

env:
  ZERO_HOME: ${{ github.workspace }}/.zero

jobs:
  scan:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0  # Full history for code ownership

      - name: Setup Go
        uses: actions/setup-go@v5
        with:
          go-version: '1.21'

      - name: Install Zero
        run: |
          go install github.com/crashappsec/zero/cmd/zero@latest

      - name: Install dependencies
        run: |
          # Install optional tools
          npm install -g @cyclonedx/cdxgen
          go install github.com/google/osv-scanner/cmd/osv-scanner@latest
          pip install semgrep

      - name: Restore cache
        uses: actions/cache@v4
        with:
          path: |
            .zero/cache
          key: zero-cache-${{ hashFiles('**/package-lock.json', '**/go.sum', '**/requirements.txt') }}

      - name: Run Zero scan
        run: |
          PROFILE="${{ github.event.inputs.profile || 'standard' }}"
          zero hydrate . --profile $PROFILE --force

      - name: Generate reports
        run: |
          zero report --repo . --format markdown --output reports/
          zero report --repo . --format sarif --output results.sarif

      - name: Upload SARIF
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: results.sarif

      - name: Upload reports
        uses: actions/upload-artifact@v4
        with:
          name: zero-reports
          path: reports/

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('reports/summary.md', 'utf8');
            github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body: report
            });
```

---

#### Task 4.2: Watch Mode Command

**File:** `cmd/zero/cmd/watch.go`

```go
package cmd

import (
    "context"
    "fmt"
    "os"
    "os/signal"
    "syscall"
    "time"

    "github.com/spf13/cobra"
    "github.com/crashappsec/zero/pkg/freshness"
    "github.com/crashappsec/zero/pkg/hydrate"
)

var watchCmd = &cobra.Command{
    Use:   "watch [repo]",
    Short: "Watch repository for changes and auto-scan",
    Long: `Continuously monitor a repository and trigger scans when:
- New commits are detected on remote
- Data becomes stale based on time threshold
- Manual trigger via signal`,
    Args: cobra.ExactArgs(1),
    RunE: runWatch,
}

func init() {
    rootCmd.AddCommand(watchCmd)

    watchCmd.Flags().Duration("interval", 1*time.Hour, "Check interval")
    watchCmd.Flags().String("profile", "standard", "Scan profile")
    watchCmd.Flags().String("on", "stale", "Trigger on: stale, very-stale, new-commits")
    watchCmd.Flags().Bool("scan-now", false, "Run initial scan immediately")
}

func runWatch(cmd *cobra.Command, args []string) error {
    projectID := args[0]
    interval, _ := cmd.Flags().GetDuration("interval")
    profile, _ := cmd.Flags().GetString("profile")
    triggerOn, _ := cmd.Flags().GetString("on")
    scanNow, _ := cmd.Flags().GetBool("scan-now")

    ctx, cancel := context.WithCancel(context.Background())
    defer cancel()

    // Handle signals
    sigCh := make(chan os.Signal, 1)
    signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM, syscall.SIGUSR1)

    fm := freshness.NewManager(zeroHome, freshness.DefaultConfig())

    fmt.Printf("Watching %s (interval: %s, trigger: %s)\n", projectID, interval, triggerOn)

    // Initial scan if requested
    if scanNow {
        runScan(ctx, projectID, profile)
    }

    ticker := time.NewTicker(interval)
    defer ticker.Stop()

    for {
        select {
        case <-ticker.C:
            meta, err := fm.CheckFreshness(projectID, true)
            if err != nil {
                fmt.Printf("Error checking freshness: %v\n", err)
                continue
            }

            shouldScan := false
            switch triggerOn {
            case "new-commits":
                shouldScan = meta.NewCommitCount > 0
            case "very-stale":
                shouldScan = meta.StalenessLevel == freshness.VeryStale
            default: // "stale"
                shouldScan = meta.StalenessLevel >= freshness.Stale
            }

            if shouldScan {
                fmt.Printf("Triggering scan: %s\n", meta.StalenessReason)
                runScan(ctx, projectID, profile)
            } else {
                fmt.Printf("[%s] %s is fresh\n",
                    time.Now().Format("15:04"), projectID)
            }

        case sig := <-sigCh:
            if sig == syscall.SIGUSR1 {
                fmt.Println("Manual scan triggered")
                runScan(ctx, projectID, profile)
            } else {
                fmt.Println("\nStopping watch...")
                return nil
            }
        }
    }
}

func runScan(ctx context.Context, projectID, profile string) {
    opts := &hydrate.Options{
        Repo:    projectID,
        Profile: profile,
        Force:   true,
    }

    h, err := hydrate.New(opts)
    if err != nil {
        fmt.Printf("Error creating hydrate: %v\n", err)
        return
    }

    _, err = h.Run(ctx)
    if err != nil {
        fmt.Printf("Scan error: %v\n", err)
    }
}
```

---

#### Task 4.3: Batch Refresh Command

**File:** `cmd/zero/cmd/refresh.go`

```go
package cmd

import (
    "context"
    "fmt"

    "github.com/spf13/cobra"
    "github.com/crashappsec/zero/pkg/freshness"
    "github.com/crashappsec/zero/pkg/hydrate"
)

var refreshCmd = &cobra.Command{
    Use:   "refresh",
    Short: "Refresh stale projects",
    Long:  `Scan all projects that have become stale based on freshness thresholds`,
    RunE:  runRefresh,
}

func init() {
    rootCmd.AddCommand(refreshCmd)

    refreshCmd.Flags().String("staleness", "stale", "Refresh projects at this staleness level or worse")
    refreshCmd.Flags().String("profile", "standard", "Scan profile to use")
    refreshCmd.Flags().Bool("dry-run", false, "Show what would be refreshed without running")
    refreshCmd.Flags().Int("limit", 0, "Maximum projects to refresh (0 = unlimited)")
}

func runRefresh(cmd *cobra.Command, args []string) error {
    staleness, _ := cmd.Flags().GetString("staleness")
    profile, _ := cmd.Flags().GetString("profile")
    dryRun, _ := cmd.Flags().GetBool("dry-run")
    limit, _ := cmd.Flags().GetInt("limit")

    fm := freshness.NewManager(zeroHome, freshness.DefaultConfig())

    // List all projects
    projects, err := listProjects(zeroHome)
    if err != nil {
        return err
    }

    var toRefresh []string

    for _, projectID := range projects {
        meta, err := fm.CheckFreshness(projectID, true)
        if err != nil {
            continue
        }

        needsRefresh := false
        switch staleness {
        case "very-stale":
            needsRefresh = meta.StalenessLevel == freshness.VeryStale
        default:
            needsRefresh = meta.StalenessLevel >= freshness.Stale
        }

        if needsRefresh {
            toRefresh = append(toRefresh, projectID)
            fmt.Printf("  %s - %s (%s)\n",
                projectID,
                meta.StalenessLevel,
                meta.StalenessReason)
        }
    }

    if len(toRefresh) == 0 {
        fmt.Println("All projects are fresh!")
        return nil
    }

    fmt.Printf("\nFound %d stale projects\n", len(toRefresh))

    if dryRun {
        return nil
    }

    // Apply limit
    if limit > 0 && len(toRefresh) > limit {
        toRefresh = toRefresh[:limit]
    }

    // Refresh each project
    ctx := context.Background()
    for i, projectID := range toRefresh {
        fmt.Printf("\n[%d/%d] Refreshing %s...\n", i+1, len(toRefresh), projectID)

        opts := &hydrate.Options{
            Repo:    projectID,
            Profile: profile,
            Force:   true,
        }

        h, err := hydrate.New(opts)
        if err != nil {
            fmt.Printf("  Error: %v\n", err)
            continue
        }

        _, err = h.Run(ctx)
        if err != nil {
            fmt.Printf("  Scan error: %v\n", err)
        }
    }

    return nil
}
```

---

#### Task 4.4: Cron/Scheduler Integration

**File:** `docs/automation.md`

Document automation options:

```markdown
# Zero Automation Guide

## Local Automation

### Using cron (Linux/macOS)

```bash
# Edit crontab
crontab -e

# Add daily scan at 2 AM
0 2 * * * /usr/local/bin/zero refresh --staleness stale --profile standard

# Add weekly full scan on Sundays
0 3 * * 0 /usr/local/bin/zero refresh --staleness fresh --profile full
```

### Using launchd (macOS)

Create `~/Library/LaunchAgents/com.zero.refresh.plist`:

```xml
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE plist PUBLIC "-//Apple//DTD PLIST 1.0//EN" "http://www.apple.com/DTDs/PropertyList-1.0.dtd">
<plist version="1.0">
<dict>
    <key>Label</key>
    <string>com.zero.refresh</string>
    <key>ProgramArguments</key>
    <array>
        <string>/usr/local/bin/zero</string>
        <string>refresh</string>
        <string>--staleness</string>
        <string>stale</string>
    </array>
    <key>StartCalendarInterval</key>
    <dict>
        <key>Hour</key>
        <integer>2</integer>
        <key>Minute</key>
        <integer>0</integer>
    </dict>
</dict>
</plist>
```

Load with: `launchctl load ~/Library/LaunchAgents/com.zero.refresh.plist`

## CI/CD Integration

### GitHub Actions

See `.github/workflows/zero-scan.yml` for a complete example.

### GitLab CI

```yaml
zero-scan:
  image: golang:1.21
  script:
    - go install github.com/crashappsec/zero/cmd/zero@latest
    - zero hydrate . --profile standard
    - zero report --format sarif --output gl-sast-report.json
  artifacts:
    reports:
      sast: gl-sast-report.json
```

### Jenkins

```groovy
pipeline {
    agent any
    triggers {
        cron('H 2 * * *')
    }
    stages {
        stage('Scan') {
            steps {
                sh 'zero hydrate . --profile standard'
                sh 'zero report --format markdown --output reports/'
            }
        }
    }
}
```
```

---

### Phase 4 Deliverables

| Deliverable | Files | Status |
|-------------|-------|--------|
| GitHub Actions workflow | `.github/workflows/zero-scan.yml` | New |
| Watch command | `cmd/zero/cmd/watch.go` | New |
| Refresh command | `cmd/zero/cmd/refresh.go` | New |
| Automation docs | `docs/automation.md` | New |

**Estimated Effort:** 2-3 days

---

## Phase 5: External Feed Sync & Rule Generation (During Hydrate)

### Overview

Automatically sync external data sources and regenerate Semgrep rules during hydrate, with configurable frequency to avoid excessive operations.

### Two Types of Semgrep Rules

Zero uses **two types of Semgrep rules** that need different update strategies:

| Type | Source | Location | Update Strategy |
|------|--------|----------|-----------------|
| **Generated Rules** | RAG patterns (patterns.md) | `.zero/cache/tech-id/rules/` | Regenerate when RAG files change |
| **Community Rules** | Semgrep Registry | `rag/semgrep/community-rules/` | Sync from registry on schedule |

#### Generated Rules (RAG ‚Üí Semgrep)

The `tech-id` scanner converts RAG markdown patterns to Semgrep YAML:

```
rag/technology-identification/**/patterns.md
         ‚îÇ
         ‚ñº  (rag_converter.go)
.zero/cache/tech-id/rules/
‚îú‚îÄ‚îÄ tech-discovery.yaml    # Technology detection
‚îú‚îÄ‚îÄ secrets.yaml           # Secret patterns
‚îú‚îÄ‚îÄ ai-ml.yaml             # AI/ML detection
‚îî‚îÄ‚îÄ config-files.yaml      # Config file detection
```

**Current behavior** (`pkg/scanners/tech-id/rules.go`):
- 24-hour TTL cache
- Checks if RAG files are newer than cache
- Regenerates if needed

#### Community Rules (Semgrep Registry)

External rules from Semgrep's community registry:

```
rag/semgrep/community-rules/
‚îú‚îÄ‚îÄ security/           # OWASP, CWE, secrets, injection
‚îú‚îÄ‚îÄ quality/            # Best practices, maintainability
‚îú‚îÄ‚îÄ languages/          # Language-specific rules
‚îú‚îÄ‚îÄ default/            # Default ruleset
‚îî‚îÄ‚îÄ .cache-info.json    # Cache metadata
```

**Current behavior**: Static files checked into Git, no auto-update.

### Design Principles

1. **Sync happens during hydrate** - Natural integration point, user expects network activity
2. **Configurable frequency** - User controls how often to check for updates (hourly, daily, weekly)
3. **Graceful degradation** - Sync failures don't block scans, use cached data
4. **Two-tier updates** - Generated rules (fast, local) + Community rules (slower, network)

### Implementation Tasks

#### Task 5.1: Rule Generation Manager

**File:** `pkg/rules/manager.go`

Unify rule generation for both tech-id and code-security scanners:

```go
package rules

import (
    "context"
    "fmt"
    "os"
    "path/filepath"
    "time"
)

// RuleType identifies the type of rules
type RuleType string

const (
    RuleTypeGenerated  RuleType = "generated"   // From RAG patterns
    RuleTypeCommunity  RuleType = "community"   // From Semgrep registry
)

// RuleSetConfig configures a rule set
type RuleSetConfig struct {
    Type      RuleType      `json:"type"`
    Enabled   bool          `json:"enabled"`
    Frequency SyncFrequency `json:"frequency"`
    Source    string        `json:"source,omitempty"`
}

// RulesConfig configures all rule sets
type RulesConfig struct {
    Enabled          bool          `json:"enabled"`
    DefaultFrequency SyncFrequency `json:"default_frequency"`
    CacheDir         string        `json:"cache_dir"`

    // Generated rules (from RAG)
    Generated struct {
        Enabled   bool          `json:"enabled"`
        Frequency SyncFrequency `json:"frequency"` // Check RAG freshness
        TTLHours  int           `json:"ttl_hours"` // Force regenerate after TTL
    } `json:"generated"`

    // Community rules (from Semgrep registry)
    Community struct {
        Enabled   bool          `json:"enabled"`
        Frequency SyncFrequency `json:"frequency"`
        Profiles  []string      `json:"profiles"` // security, quality, languages
        Source    string        `json:"source"`   // Registry URL
    } `json:"community"`
}

// DefaultRulesConfig returns sensible defaults
func DefaultRulesConfig() RulesConfig {
    return RulesConfig{
        Enabled:          true,
        DefaultFrequency: FreqDaily,
        CacheDir:         ".zero/cache/rules",
        Generated: struct {
            Enabled   bool          `json:"enabled"`
            Frequency SyncFrequency `json:"frequency"`
            TTLHours  int           `json:"ttl_hours"`
        }{
            Enabled:   true,
            Frequency: FreqAlways, // Always check if RAG changed (fast, local)
            TTLHours:  24,
        },
        Community: struct {
            Enabled   bool          `json:"enabled"`
            Frequency SyncFrequency `json:"frequency"`
            Profiles  []string      `json:"profiles"`
            Source    string        `json:"source"`
        }{
            Enabled:   true,
            Frequency: FreqWeekly, // Community rules don't change often
            Profiles:  []string{"security", "quality", "languages"},
            Source:    "https://semgrep.dev/c/r",
        },
    }
}

// RuleManager handles rule lifecycle
type RuleManager struct {
    config    RulesConfig
    ragPath   string
    cacheDir  string
    onStatus  func(string)
}

// NewRuleManager creates a new rule manager
func NewRuleManager(config RulesConfig, ragPath string, onStatus func(string)) *RuleManager {
    if onStatus == nil {
        onStatus = func(string) {}
    }
    return &RuleManager{
        config:   config,
        ragPath:  ragPath,
        cacheDir: config.CacheDir,
        onStatus: onStatus,
    }
}

// RefreshResult contains the result of a refresh operation
type RefreshResult struct {
    GeneratedRules struct {
        Refreshed  bool
        RuleCount  int
        FromCache  bool
        Error      error
    }
    CommunityRules struct {
        Refreshed  bool
        RuleCount  int
        FromCache  bool
        Error      error
    }
}

// Refresh checks and updates all rules based on config
func (rm *RuleManager) Refresh(ctx context.Context, force bool) *RefreshResult {
    result := &RefreshResult{}

    // 1. Generated rules (from RAG patterns)
    if rm.config.Generated.Enabled {
        rm.onStatus("Checking generated rules...")
        result.GeneratedRules = rm.refreshGeneratedRules(ctx, force)
    }

    // 2. Community rules (from Semgrep registry)
    if rm.config.Community.Enabled && rm.shouldSyncCommunity() {
        rm.onStatus("Syncing community rules...")
        result.CommunityRules = rm.refreshCommunityRules(ctx, force)
    }

    return result
}

// refreshGeneratedRules regenerates rules from RAG patterns if needed
func (rm *RuleManager) refreshGeneratedRules(ctx context.Context, force bool) struct {
    Refreshed bool
    RuleCount int
    FromCache bool
    Error     error
} {
    // Use existing RuleManager from tech-id package
    // This already handles:
    // - TTL-based expiration
    // - RAG file modification detection
    // - Semgrep YAML generation

    // See: pkg/scanners/tech-id/rules.go
    // The existing implementation is good, just integrate it here

    return struct {
        Refreshed bool
        RuleCount int
        FromCache bool
        Error     error
    }{}
}

// refreshCommunityRules syncs rules from Semgrep registry
func (rm *RuleManager) refreshCommunityRules(ctx context.Context, force bool) struct {
    Refreshed bool
    RuleCount int
    FromCache bool
    Error     error
} {
    // Download from Semgrep registry
    // Pre-approved URLs:
    // - https://semgrep.dev/c/r/security
    // - https://semgrep.dev/c/r/quality
    // - https://semgrep.dev/c/r/all

    return struct {
        Refreshed bool
        RuleCount int
        FromCache bool
        Error     error
    }{}
}

func (rm *RuleManager) shouldSyncCommunity() bool {
    freq := rm.config.Community.Frequency
    if freq == FreqNever {
        return false
    }
    if freq == FreqAlways {
        return true
    }

    // Check last sync time
    cacheInfo := filepath.Join(rm.cacheDir, "community", ".cache-info.json")
    info, err := os.Stat(cacheInfo)
    if err != nil {
        return true // No cache = sync needed
    }

    elapsed := time.Since(info.ModTime())
    switch freq {
    case FreqHourly:
        return elapsed >= time.Hour
    case FreqDaily:
        return elapsed >= 24*time.Hour
    case FreqWeekly:
        return elapsed >= 7*24*time.Hour
    default:
        return true
    }
}
```

---

#### Task 5.2: Feed Sync Types

**File:** `pkg/feeds/types.go`

> Note: This was renumbered from the original 4.1 to make room for the Rule Generation Manager.

```go
package feeds

import (
    "time"
)

// FeedType identifies the type of external feed
type FeedType string

const (
    // NOTE: OSV is NOT a cached feed - it's queried LIVE during scans
    // via https://api.osv.dev/v1/query (pre-approved URL)

    FeedSemgrep     FeedType = "semgrep"       // Semgrep community rules
    FeedGitHubAdv   FeedType = "github-advisories"
    FeedNVD         FeedType = "nvd"           // NVD CVE database (optional)
)

// SyncFrequency controls how often feeds are updated
type SyncFrequency string

const (
    FreqNever    SyncFrequency = "never"     // Disable auto-sync
    FreqAlways   SyncFrequency = "always"    // Sync on every hydrate
    FreqHourly   SyncFrequency = "hourly"    // Max once per hour
    FreqDaily    SyncFrequency = "daily"     // Max once per day (default)
    FreqWeekly   SyncFrequency = "weekly"    // Max once per week
)

// FeedConfig configures a single feed
type FeedConfig struct {
    Type      FeedType      `json:"type"`
    Enabled   bool          `json:"enabled"`
    Frequency SyncFrequency `json:"frequency"`
    Source    string        `json:"source,omitempty"`    // Override default source URL
    Timeout   int           `json:"timeout_seconds"`     // Request timeout
}

// FeedsConfig configures all external feeds
type FeedsConfig struct {
    // Global settings
    Enabled         bool          `json:"enabled"`           // Master switch
    DefaultFrequency SyncFrequency `json:"default_frequency"` // Default for all feeds
    CacheDir        string        `json:"cache_dir"`         // Where to store cached data

    // Per-feed configuration
    OSV            *FeedConfig `json:"osv,omitempty"`
    NVD            *FeedConfig `json:"nvd,omitempty"`
    Semgrep        *FeedConfig `json:"semgrep,omitempty"`
    GitHubAdvisories *FeedConfig `json:"github_advisories,omitempty"`

    // Behavior
    FailOnSyncError bool `json:"fail_on_sync_error"` // Stop hydrate if sync fails
    ShowProgress    bool `json:"show_progress"`       // Show sync progress
}

// FeedStatus tracks the state of a feed
type FeedStatus struct {
    Type          FeedType  `json:"type"`
    LastSyncedAt  time.Time `json:"last_synced_at"`
    LastCheckAt   time.Time `json:"last_check_at"`
    Version       string    `json:"version,omitempty"`       // Feed version/etag
    RecordCount   int       `json:"record_count,omitempty"`
    SizeBytes     int64     `json:"size_bytes,omitempty"`
    Error         string    `json:"error,omitempty"`
    Stale         bool      `json:"stale"`
}

// FeedsStatus tracks all feed states
type FeedsStatus struct {
    Feeds     map[FeedType]*FeedStatus `json:"feeds"`
    UpdatedAt time.Time                `json:"updated_at"`
}

// DefaultFeedsConfig returns sensible defaults
func DefaultFeedsConfig() FeedsConfig {
    return FeedsConfig{
        Enabled:          true,
        DefaultFrequency: FreqDaily,
        CacheDir:         ".zero/cache/feeds",
        FailOnSyncError:  false,
        ShowProgress:     true,
        // NOTE: OSV is queried LIVE during scans, not cached here.
        // Pre-approved URL: https://api.osv.dev/v1/query
        Semgrep: &FeedConfig{
            Type:      FeedSemgrep,
            Enabled:   true,
            Frequency: FreqWeekly,  // Rules don't change as often
            Source:    "https://semgrep.dev/c/r/all",
            Timeout:   120,
        },
        GitHubAdvisories: &FeedConfig{
            Type:      FeedGitHubAdv,
            Enabled:   true,
            Frequency: FreqDaily,
            Source:    "https://github.com/github/advisory-database",
            Timeout:   60,
        },
    }
}
```

---

#### Task 5.3: Feed Sync Manager

**File:** `pkg/feeds/manager.go`

```go
package feeds

import (
    "encoding/json"
    "fmt"
    "os"
    "path/filepath"
    "time"
)

// Manager handles external feed synchronization
type Manager struct {
    config   FeedsConfig
    cacheDir string
    status   *FeedsStatus
}

// NewManager creates a new feed sync manager
func NewManager(config FeedsConfig) (*Manager, error) {
    cacheDir := config.CacheDir
    if cacheDir == "" {
        cacheDir = ".zero/cache/feeds"
    }

    // Ensure cache directory exists
    if err := os.MkdirAll(cacheDir, 0755); err != nil {
        return nil, fmt.Errorf("creating cache dir: %w", err)
    }

    m := &Manager{
        config:   config,
        cacheDir: cacheDir,
    }

    // Load existing status
    m.loadStatus()

    return m, nil
}

// ShouldSync checks if a feed should be synced based on frequency
func (m *Manager) ShouldSync(feedType FeedType) bool {
    if !m.config.Enabled {
        return false
    }

    feedCfg := m.getFeedConfig(feedType)
    if feedCfg == nil || !feedCfg.Enabled {
        return false
    }

    if feedCfg.Frequency == FreqNever {
        return false
    }

    if feedCfg.Frequency == FreqAlways {
        return true
    }

    // Check last sync time
    status := m.status.Feeds[feedType]
    if status == nil {
        return true // Never synced
    }

    elapsed := time.Since(status.LastSyncedAt)

    switch feedCfg.Frequency {
    case FreqHourly:
        return elapsed >= time.Hour
    case FreqDaily:
        return elapsed >= 24*time.Hour
    case FreqWeekly:
        return elapsed >= 7*24*time.Hour
    default:
        return true
    }
}

// SyncAll syncs all enabled feeds that are due for update
// NOTE: OSV is queried LIVE during scans, not synced here
func (m *Manager) SyncAll(onProgress func(feed FeedType, status string)) error {
    if !m.config.Enabled {
        return nil
    }

    feeds := []FeedType{FeedSemgrep, FeedGitHubAdv}

    for _, feed := range feeds {
        if !m.ShouldSync(feed) {
            continue
        }

        if onProgress != nil {
            onProgress(feed, "syncing")
        }

        err := m.syncFeed(feed)
        if err != nil {
            if onProgress != nil {
                onProgress(feed, fmt.Sprintf("error: %v", err))
            }
            if m.config.FailOnSyncError {
                return fmt.Errorf("syncing %s: %w", feed, err)
            }
            // Record error but continue
            m.recordError(feed, err)
        } else {
            if onProgress != nil {
                onProgress(feed, "complete")
            }
        }
    }

    return m.saveStatus()
}

// SyncFeed syncs a specific feed
// NOTE: OSV is queried LIVE, not synced
func (m *Manager) syncFeed(feedType FeedType) error {
    switch feedType {
    case FeedSemgrep:
        return m.syncSemgrep()
    case FeedGitHubAdv:
        return m.syncGitHubAdvisories()
    default:
        return fmt.Errorf("unknown feed type: %s", feedType)
    }
}

// NOTE: OSV is queried LIVE during scans, not cached.
// See pkg/scanners/package-analysis for live OSV API integration.
// The OSV API (https://api.osv.dev/v1/query) should be pre-approved
// in the allow list to avoid user interaction prompts.
//
// TODO: Remove OSV from feed sync - it's not a cached feed.
// OSV queries happen in real-time during package-analysis scans.

// syncSemgrep syncs Semgrep community rules
func (m *Manager) syncSemgrep() error {
    // Check if we have rules and they're recent enough
    rulesDir := filepath.Join(m.cacheDir, "semgrep", "rules")

    // Use semgrep CLI to update registry
    // Or download from GitHub releases

    m.recordSuccess(FeedSemgrep)
    return nil
}

// syncGitHubAdvisories syncs GitHub Advisory Database
func (m *Manager) syncGitHubAdvisories() error {
    // Clone/update the advisory database
    // https://github.com/github/advisory-database

    m.recordSuccess(FeedGitHubAdv)
    return nil
}

// Helper functions

func (m *Manager) getFeedConfig(feedType FeedType) *FeedConfig {
    switch feedType {
    case FeedOSV:
        return m.config.OSV
    case FeedSemgrep:
        return m.config.Semgrep
    case FeedGitHubAdv:
        return m.config.GitHubAdvisories
    default:
        return nil
    }
}

func (m *Manager) loadStatus() {
    statusFile := filepath.Join(m.cacheDir, "status.json")
    data, err := os.ReadFile(statusFile)
    if err != nil {
        m.status = &FeedsStatus{
            Feeds: make(map[FeedType]*FeedStatus),
        }
        return
    }

    var status FeedsStatus
    if err := json.Unmarshal(data, &status); err != nil {
        m.status = &FeedsStatus{
            Feeds: make(map[FeedType]*FeedStatus),
        }
        return
    }
    m.status = &status
}

func (m *Manager) saveStatus() error {
    m.status.UpdatedAt = time.Now()

    statusFile := filepath.Join(m.cacheDir, "status.json")
    data, err := json.MarshalIndent(m.status, "", "  ")
    if err != nil {
        return err
    }
    return os.WriteFile(statusFile, data, 0644)
}

func (m *Manager) recordSuccess(feedType FeedType) {
    if m.status.Feeds == nil {
        m.status.Feeds = make(map[FeedType]*FeedStatus)
    }
    m.status.Feeds[feedType] = &FeedStatus{
        Type:         feedType,
        LastSyncedAt: time.Now(),
        LastCheckAt:  time.Now(),
        Stale:        false,
    }
}

func (m *Manager) recordError(feedType FeedType, err error) {
    if m.status.Feeds == nil {
        m.status.Feeds = make(map[FeedType]*FeedStatus)
    }

    existing := m.status.Feeds[feedType]
    if existing == nil {
        existing = &FeedStatus{Type: feedType}
    }

    existing.LastCheckAt = time.Now()
    existing.Error = err.Error()
    existing.Stale = true
    m.status.Feeds[feedType] = existing
}

func (m *Manager) downloadFile(url, dest string) error {
    // Ensure directory exists
    if err := os.MkdirAll(filepath.Dir(dest), 0755); err != nil {
        return err
    }

    // TODO: Implement with proper HTTP client, etag caching, etc.
    return nil
}

// GetCacheDir returns the cache directory for a specific feed
func (m *Manager) GetCacheDir(feedType FeedType) string {
    return filepath.Join(m.cacheDir, string(feedType))
}

// GetStatus returns the current status of all feeds
func (m *Manager) GetStatus() *FeedsStatus {
    return m.status
}
```

---

#### Task 5.4: Hydrate Integration

**File:** `pkg/hydrate/hydrate.go` (update)

Add feed sync at the beginning of hydrate:

```go
// In the Run() function, before cloning/scanning:

func (h *Hydrate) Run(ctx context.Context) ([]string, error) {
    start := time.Now()

    // Sync external feeds if configured
    if err := h.syncFeeds(ctx); err != nil {
        // Log but don't fail unless configured to
        h.term.Warn("Feed sync warning: %v", err)
    }

    // ... rest of existing Run() logic
}

func (h *Hydrate) syncFeeds(ctx context.Context) error {
    feedsCfg := h.cfg.GetFeedsConfig()
    if !feedsCfg.Enabled {
        return nil
    }

    fm, err := feeds.NewManager(feedsCfg)
    if err != nil {
        return err
    }

    // Check what needs syncing
    // NOTE: OSV is queried LIVE during scans, not synced here
    var needsSync []feeds.FeedType
    for _, feed := range []feeds.FeedType{feeds.FeedSemgrep, feeds.FeedGitHubAdv} {
        if fm.ShouldSync(feed) {
            needsSync = append(needsSync, feed)
        }
    }

    if len(needsSync) == 0 {
        return nil
    }

    h.term.Info("Syncing external feeds...")

    return fm.SyncAll(func(feed feeds.FeedType, status string) {
        switch status {
        case "syncing":
            h.term.Info("  %s: syncing...", feed)
        case "complete":
            h.term.Info("  %s: %s", feed, h.term.Color(terminal.Green, "complete"))
        default:
            h.term.Warn("  %s: %s", feed, status)
        }
    })
}
```

---

#### Task 5.5: CLI Control

**File:** `cmd/zero/cmd/feeds.go`

```go
package cmd

import (
    "fmt"

    "github.com/spf13/cobra"
    "github.com/crashappsec/zero/pkg/feeds"
)

var feedsCmd = &cobra.Command{
    Use:   "feeds",
    Short: "Manage external data feeds",
    Long:  `View and control external feed synchronization (CVE databases, Semgrep rules, etc.)`,
}

var feedsStatusCmd = &cobra.Command{
    Use:   "status",
    Short: "Show feed sync status",
    RunE: func(cmd *cobra.Command, args []string) error {
        cfg := loadConfig()
        fm, err := feeds.NewManager(cfg.GetFeedsConfig())
        if err != nil {
            return err
        }

        status := fm.GetStatus()
        fmt.Println("Feed Status:")
        fmt.Println()

        for feedType, feedStatus := range status.Feeds {
            staleMarker := ""
            if feedStatus.Stale {
                staleMarker = " (STALE)"
            }

            fmt.Printf("  %s%s\n", feedType, staleMarker)
            fmt.Printf("    Last synced: %s\n", feedStatus.LastSyncedAt.Format("2006-01-02 15:04"))
            if feedStatus.Error != "" {
                fmt.Printf("    Error: %s\n", feedStatus.Error)
            }
        }

        return nil
    },
}

var feedsSyncCmd = &cobra.Command{
    Use:   "sync [feed]",
    Short: "Force sync feeds",
    Long:  `Force synchronization of external feeds regardless of frequency settings`,
    RunE: func(cmd *cobra.Command, args []string) error {
        cfg := loadConfig()
        feedsCfg := cfg.GetFeedsConfig()
        feedsCfg.DefaultFrequency = feeds.FreqAlways // Force sync

        fm, err := feeds.NewManager(feedsCfg)
        if err != nil {
            return err
        }

        fmt.Println("Syncing feeds...")
        return fm.SyncAll(func(feed feeds.FeedType, status string) {
            fmt.Printf("  %s: %s\n", feed, status)
        })
    },
}

func init() {
    rootCmd.AddCommand(feedsCmd)
    feedsCmd.AddCommand(feedsStatusCmd)
    feedsCmd.AddCommand(feedsSyncCmd)
}
```

---

#### Task 5.6: Configuration

Add to `config/zero.config.json`:

```json
{
  "feeds": {
    "enabled": true,
    "default_frequency": "daily",
    "cache_dir": ".zero/cache/feeds",
    "fail_on_sync_error": false,
    "show_progress": true,
    "semgrep": {
      "enabled": true,
      "frequency": "weekly"
    },
    "github_advisories": {
      "enabled": true,
      "frequency": "daily"
    }
  }
}
```

**Note:** OSV is queried LIVE during scans (not cached) via the OSV API.

---

#### Task 5.7: Pre-Approved URLs

The following URLs should be pre-approved in Claude Code settings to avoid user interaction prompts during automated scans:

**File:** `.claude/settings.json` (or user settings)

```json
{
  "permissions": {
    "allow": [
      "WebFetch(https://api.osv.dev/*)",
      "WebFetch(https://semgrep.dev/*)",
      "WebFetch(https://raw.githubusercontent.com/github/advisory-database/*)",
      "WebFetch(https://osv-vulnerabilities.storage.googleapis.com/*)",
      "WebFetch(https://registry.npmjs.org/*)",
      "WebFetch(https://pypi.org/pypi/*)"
    ]
  }
}
```

**Pre-Approved URL List:**

| URL Pattern | Purpose | Used By |
|-------------|---------|---------|
| `https://api.osv.dev/v1/query` | Live vulnerability lookup | package-analysis scanner |
| `https://api.osv.dev/v1/vulns/*` | Vulnerability details | package-analysis scanner |
| `https://semgrep.dev/c/r/*` | Semgrep rule registry | code-security scanner |
| `https://raw.githubusercontent.com/github/advisory-database/*` | GitHub advisories | feed sync |
| `https://registry.npmjs.org/*` | npm package metadata | package-analysis scanner |
| `https://pypi.org/pypi/*/json` | PyPI package metadata | package-analysis scanner |

These URLs are for security scanning purposes and should not require user confirmation.

---

### Phase 5 Deliverables

| Deliverable | Files | Status |
|-------------|-------|--------|
| Rule generation manager | `pkg/rules/manager.go` | New |
| Feed types | `pkg/feeds/types.go` | New |
| Feed manager | `pkg/feeds/manager.go` | New |
| Semgrep sync | `pkg/feeds/semgrep.go` | New |
| GitHub advisories sync | `pkg/feeds/github.go` | New |
| Hydrate integration | `pkg/hydrate/hydrate.go` | Modify |
| Feeds CLI | `cmd/zero/cmd/feeds.go` | New |
| Config updates | `pkg/config/config.go` | Modify |
| Pre-approved URLs | `.claude/settings.json` | Modify |

**Note:** OSV is queried LIVE during scans (in package-analysis scanner), not synced as a feed. The `pkg/liveapi/` package from Phase 1 provides the infrastructure.

**Estimated Effort:** 3-4 days (includes rule generation + feed sync)

---

## Configuration Updates

### Updated zero.config.json

```json
{
  "version": "3.5.0",
  "settings": {
    "default_profile": "standard",
    "storage_path": ".zero",
    "parallel_repos": 8,
    "parallel_scanners": 4,
    "scanner_timeout_seconds": 300,
    "cache_ttl_hours": 24
  },
  "freshness": {
    "fresh_max_hours": 24,
    "stale_max_days": 7,
    "fresh_max_commits": 0,
    "stale_max_commits": 10,
    "auto_refresh_on_stale": false,
    "check_remote_on_status": true
  },
  "reports": {
    "auto_generate": true,
    "formats": ["markdown"],
    "include_technical": true,
    "include_executive": true
  },
  "automation": {
    "watch_interval_hours": 1,
    "refresh_trigger": "stale",
    "max_parallel_refresh": 4
  },
  "feeds": {
    "enabled": true,
    "default_frequency": "daily",
    "cache_dir": ".zero/cache/feeds",
    "fail_on_sync_error": false,
    "show_progress": true,
    "semgrep": {
      "enabled": true,
      "frequency": "weekly"
    },
    "github_advisories": {
      "enabled": true,
      "frequency": "daily"
    }
  },
  "live_apis": {
    "osv": {
      "enabled": true,
      "base_url": "https://api.osv.dev/v1",
      "timeout_seconds": 30
    }
  }
}
```

---

## Testing Plan

### Phase 1 Tests

1. **Unit tests for report builder** (`pkg/report/generator_test.go`)
2. **Report generation tests for each scanner** (`pkg/scanners/*/report_test.go`)
3. **Integration test for combined reports**

### Phase 2 Tests

1. **Freshness calculation tests** (`pkg/freshness/manager_test.go`)
2. **Staleness threshold tests**
3. **Git remote checking tests**

### Phase 3 Tests

1. **Watch mode tests** (mock ticker)
2. **Refresh command tests**
3. **GitHub Actions workflow validation**

---

## Migration Notes

### For Existing Users

1. Run `zero status` to see freshness info (Phase 2)
2. Existing analysis data will show as "unknown" freshness until next scan
3. No breaking changes to existing commands

### Deprecations

- None planned

---

## Summary

| Phase | Focus | Effort | Key Deliverable |
|-------|-------|--------|-----------------|
| 1 | Shared Libraries | 2-3 days | `pkg/report/`, `pkg/findings/`, `pkg/scoring/`, `pkg/rag/`, `pkg/liveapi/` |
| 2 | Report Generation | 2-4 days | Markdown reports for all 9 scanners (using shared libs) |
| 3 | Staleness Detection | 2-3 days | Freshness metadata + conditional hydrate |
| 4 | Automation | 2-3 days | Watch mode + GitHub Actions + refresh command |
| 5 | External Feed Sync & Rule Generation | 3-4 days | Rule generation + Semgrep/GitHub sync + OSV live API |

**Total Estimated Effort:** 11-17 days (shared libs reduce report generation effort)

---

## Implementation Order

1. **Phase 1: Shared Libraries** - Build foundation first to avoid refactoring later
2. **Phase 2: Report Generation** - Uses shared libs, faster implementation
3. **Phase 3: Staleness Detection** - Foundation for knowing when data is outdated
4. **Phase 4: Automation** - Watch mode, scheduled scans, GitHub Actions
5. **Phase 5: External Feed Sync** - Semgrep/GitHub Advisory sync during hydrate + OSV live queries

**Rationale:** Building shared libraries first means:
- Report generation (Phase 2) goes faster since framework already exists
- All 9 scanners use consistent patterns from day one
- No refactoring needed when adding new scanners later
- RAG and Live API infrastructure ready for Phase 5

---

## Quick Reference: New Commands

After implementation, users will have these new commands:

```bash
# Report generation
zero report --repo owner/repo --technical    # Generate technical reports
zero report --repo owner/repo --executive    # Generate executive reports
zero report --repo owner/repo --all          # Generate all report types

# Freshness checking
zero status                                   # Shows freshness status
zero hydrate owner/repo --if-stale           # Only scan if data is stale

# Automation
zero watch owner/repo --interval 1h          # Continuous monitoring
zero refresh --staleness stale               # Refresh all stale projects

# Feed management
zero feeds status                            # Show feed sync status
zero feeds sync                              # Force sync all feeds
```

---

## Quick Reference: Configuration Options

```bash
# In zero.config.json or via CLI flags:

# Report generation
--reports.auto_generate=true                 # Auto-generate reports after scan
--reports.formats=markdown,html              # Output formats

# Freshness thresholds
--freshness.fresh_max_hours=24               # Fresh if scanned within 24h
--freshness.stale_max_days=7                 # Very stale after 7 days

# Feed sync frequency (Semgrep, GitHub Advisories - OSV is live)
--feeds.default_frequency=daily              # Default: daily
--feeds.semgrep.frequency=weekly             # Semgrep: sync weekly
--feeds.github_advisories.frequency=daily    # GitHub: sync daily

# Live API settings (OSV)
--live_apis.osv.enabled=true                 # Enable live OSV lookups
--live_apis.osv.timeout_seconds=30           # API timeout
```

---

*Plan created: 2025-12-19*
*Author: Zero Architecture Team*
